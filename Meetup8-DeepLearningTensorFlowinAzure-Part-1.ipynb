{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 align=\"center\">Deep learning with TensorFlow in Azure PART 1</h1>\n",
    "<h1 align=\"center\">Introduction to Deep Learning</h1>\n",
    "<h1 align=\"center\">Meetup DFW Data & AI - Microsoft</h1>\n",
    "## Setting Up Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1) Deploy Linux Data Science VM in Azure - Ubuntu version with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to complete this notebook, you must deploy a Linux DSVM in your azure subscription. Click [HERE](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.linux-data-science-vm-ubuntu), then click on GET IT NOW.\n",
    "\n",
    "**In Basics blade:**<br>\n",
    "**Name:** meetupdsvmgpu <br>\n",
    "**VM Disk type:** HDD<br>\n",
    "**Username:** sshuser<br>\n",
    "**Password:** Passw0rd.1!!<br>\n",
    "**Resource Group:** meetupdsvmgpu_rg <br>\n",
    "**Location:** Pick among East US, North Central US, South Central US or West US 2<br>\n",
    "\n",
    "**In Size blade:\n",
    "Size:** NC6 (if you want GPU), or D4_V3 (if you want CPU)\n",
    "\n",
    "**In Settings blade:**\n",
    "Leave as is\n",
    "\n",
    "\n",
    "[The data science VM](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-virtual-machine-overview) can be used for training model using deep learning algorithms on GPU (Graphics processing units) based hardware. Utilizing VM scaling capabilites of Azure cloud, DSVM helps you use GPU based hardware on the cloud as per need. One can switch to a GPU based VM when training large models or need high speed computations while keeping the same OS disk. The Windows Server 2016 edition of DSVM comes pre-installed with GPU drivers, frameworks and GPU version of the deep learning algorithms. On the Linux, deep learning on GPU is enabled only on the Data Science Virtual Machine for Linux (Ubuntu) edition. You can deploy the Ubuntu/Windows-2016 edition of Data Science VM to non GPU based Azure virtual machine in which case all the deep learning frameworks will fallback to the CPU mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2) SSH into the VM and git clone the meetup repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```\n",
    "> ssh sshuser@YOUR.VM.IP.ADDRESS\n",
    "\n",
    "> cd notebooks\n",
    "\n",
    "> git clone https://github.com/pablomarin/Meetups-Data-AI-DFW.git\n",
    "\n",
    "> sudo ln -s /anaconda/envs/py35/bin/pip /usr/bin/pip3\n",
    "\n",
    "> sudo pip3 install tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3) Open the Jupyter notebook from your VM on your local browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> https://YOUR.VM.IP.ADDRESS:8000 <br>\n",
    "> Login with your VM username and password<br>\n",
    "> Go to the ***Meetups-Data-AI-DFW folder***<br>\n",
    "> Open the Notebook:***Meetup8-DeepLearningTensorFlowinAzure-Part-1.ipynb***<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## PART 1 - INTRO TO DEEPLEARNING IN AZURE USING TENSORFLOW\n",
    "**Disclaimer:** Most of the below information was originally created by Udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'm going to divide this talk into three sections:\n",
    "1. WHAT - What is Deep Learning\n",
    "2. WHERE - Where I can run Deep Learning\n",
    "3. HOW - How do I run Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1) WHAT  - What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "![img](image/machine-learning-deep-learning-and-data-analysis-introduction-4-638.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![img](image/ai-ml-dl.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### My Personal Definition:\n",
    "DeepLearning = ( Matrix Multiplication + Gradient Descent ) * Multiple times back and forth on Vast amount Data using GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6 Basics concepts to know to understand Deep Learning (Neural Networks with many layers):\n",
    "1. Perceptron (neuron)\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=Mqogpnp1lrU\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/Mqogpnp1lrU/1.jpg\" alt=\"IMAGE\" width=\"240\" height=\"180\" border=\"10\" /></a>\n",
    "2. Gradient Descent\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=7sxA5Ap8AWM\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/7sxA5Ap8AWM/0.jpg\" alt=\"IMAGE\" width=\"240\" height=\"180\" border=\"10\" /></a>\n",
    "3. Multilayer Perceptron\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=Rs9petvTBLk\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/Rs9petvTBLk/0.jpg\" alt=\"IMAGE\" width=\"240\" height=\"180\" border=\"10\" /></a>\n",
    "4. Backpropagation\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=MZL97-2joxQ\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/MZL97-2joxQ/0.jpg\" alt=\"IMAGE\" width=\"240\" height=\"180\" border=\"10\" /></a>\n",
    "4. Deep vs Wide Architecture\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=CsB7yUtMJyk\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/CsB7yUtMJyk/0.jpg\" alt=\"IMAGE\" width=\"240\" height=\"180\" border=\"10\" /></a>\n",
    "4. Regularization & Dropout\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=6DcImJS8uV8\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/6DcImJS8uV8/0.jpg\" alt=\"IMAGE\" width=\"240\" height=\"180\" border=\"10\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### A neural network using only Numpy. To solve the college admission classification problem on 1. before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.25135725242598617\n",
      "Train loss:  0.24996540718842886\n",
      "Train loss:  0.24862005218904654\n",
      "Train loss:  0.24731993217179746\n",
      "Train loss:  0.24606380465584848\n",
      "Train loss:  0.24485044179257162\n",
      "Train loss:  0.2436786320186832\n",
      "Train loss:  0.24254718151769536\n",
      "Train loss:  0.24145491550165465\n",
      "Train loss:  0.24040067932493367\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(21)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "        output = sigmoid(np.dot(hidden_output,\n",
    "                                weights_hidden_output))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # Calculate the network's prediction error\n",
    "        error = y - output\n",
    "\n",
    "        # Calculate error term for the output unit\n",
    "        output_error_term = error * output * (1 - output)\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "\n",
    "        # Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
    "\n",
    "        # Calculate the error term for the hidden layer\n",
    "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "\n",
    "        # Update the change in weights\n",
    "        del_w_hidden_output += output_error_term * hidden_output\n",
    "        del_w_input_hidden += hidden_error_term * x[:, None]\n",
    "\n",
    "    # Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2) WHERE - Azure as the Infrastructure for Deep Learning\n",
    "We just created before a multi-layer perceptron using just Numpy to solve a very small classification problem.\n",
    "The reality though, is that problems are much more complex, and the accuracy required must be equal or better than human standard. In order to achieve this, we need three main things: a Massive compute and storage platform with GPUs (like Azure),  a Deep Learning framework/library like CNTK or TensorFlow, and a lot of Data.\n",
    "\n",
    "As of August 2017, This is what Microsoft Azure has to offer regarding Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![img](image/Azure-GPU-Roadmap.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![img](image/GPU-NCSeries.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![img](image/GPU-NDSeries.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![img](image/DSVM-DeepLearning.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![img](image/Msft-Cognitive-Services-API.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![img](image/Azure-Batch-AI.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3) HOW - Using TensorFlow (A Deep Learning backend library) in Azure DSVM ![tf](https://avatars-05.gitter.im/group/iv/3/57542c04c43b8c601976f1a5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Throughout this lesson, you'll apply your knowledge of neural networks on real datasets using [TensorFlow](http://tensorflow.org), an open source Deep Learning library.\n",
    "You’ll use TensorFlow to classify images from the notMNIST dataset - a dataset of images of English letters from A to J."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "But before we start classifing images, let's first understand a little bit of Tensorflow. Starting with a Hello World example, and some basic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hello, world!\n",
    "Try running the following code to make sure you have TensorFlow properly installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n",
      "b'Hello World!'\n",
      "123\n",
      "4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:9: UserWarning: No GPU found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65900117  0.24243298  0.09856589]\n",
      "0.41703\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "# Create TensorFlow objects called tensors\n",
    "# String Constant\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "# Number Constants\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "\n",
    "# Vector Constants\n",
    "logit_data = [2.0, 1.0, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "#Define later-set constants, so you can reuse the same constant name in the code\n",
    "n = tf.placeholder(tf.int32)\n",
    "logits = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "output = None\n",
    "\n",
    "#Define Variables\n",
    "v = tf.Variable(5)\n",
    "\n",
    "# Define Functions \n",
    "z = tf.subtract(tf.divide(x,y),tf.cast(tf.constant(1), tf.float64)) # z = x/y - 1\n",
    "softmax = tf.nn.softmax(logits)\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "# Create Session and run commands\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)\n",
    "    \n",
    "    output = sess.run(n, feed_dict={n: 123})\n",
    "    print(output)\n",
    "    \n",
    "    output = sess.run(z)\n",
    "    print(output)\n",
    "    \n",
    "    softmax_data = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "    print(softmax_data)\n",
    "    \n",
    "    output = sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 10 API calls to know to understand the basics of TensorFlow:\n",
    "1. **Tensor**:<br>\n",
    "In TensorFlow, data isn’t stored as integers, floats, or strings. These values are encapsulated in an object called a tensor. In the case of hello_constant = tf.constant('Hello World!'), hello_constant is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below:<br>\n",
    "> A is a 0-dimensional int32 tensor<br>\n",
    "A = tf.constant(1234) <br>\n",
    " B is a 1-dimensional int32 tensor<br>\n",
    "B = tf.constant([123,456,789]) <br>\n",
    " C is a 2-dimensional int32 tensor<br>\n",
    "C = tf.constant([ [123,456,789], [222,333,444] ])<br>\n",
    "\n",
    "2. **Session**: <br>\n",
    "TensorFlow’s api is built around the idea of a computational graph. A session is an environment for running a graph. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines.\n",
    "\n",
    "3. **tf.placeholder()**: <br>\n",
    "Sadly you can’t just set x to your dataset and put it in TensorFlow, because over time you'll want your TensorFlow model to take in different datasets with different parameters. You need tf.placeholder()! <br>\n",
    "tf.placeholder() returns a tensor that gets its value from data passed to the tf.session.run() function, allowing you to set the input right before the session runs.\n",
    "\n",
    "4. **Math functions**: <br>\n",
    ">x = tf.add(5, 2)  # 7 <br>\n",
    "x = tf.subtract(40, 30) # 10 <br>\n",
    "y = tf.multiply(1, 5)  # 5 - Element wise multiplication<br>\n",
    "z = tf.divide(x,y) # 2 <br>\n",
    "m = [tf.matmul(a,b)](https://www.tensorflow.org/api_docs/python/tf/matmul) # Multiplies matrix a by matrix b, producing a * b (dot product) <br>\n",
    "s = [tf.reduce_sum()](https://www.tensorflow.org/api_docs/python/tf/reduce_sum) # Computes the sum of elements across dimensions of a tensor<br>\n",
    "l = [tf.log()](https://www.tensorflow.org/api_docs/python/tf/log) # Computes natural logarithm of x element-wise.\n",
    "\n",
    "5. **tf.variable()**:<br>\n",
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out tf.placeholder() and tf.constant(), since those Tensors can't be modified. This is where tf.Variable class comes in.<br>\n",
    "The tf.Variable class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. You'll use the tf.global_variables_initializer() function to initialize the state of all the Variable tensors.<br><br>\n",
    "The tf.global_variables_initializer() call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen.\n",
    "```\n",
    "init = tf.global_variables_initializer() <br>\n",
    "with tf.Session() as sess:<br>\n",
    "   sess.run(init)<br><br>\n",
    "```\n",
    "\n",
    "6. **tf.truncated_normal()**:<br>\n",
    "The [tf.truncated_normal()](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean. <br>\n",
    "```\n",
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "```\n",
    "\n",
    "7. **tf.zeros()**:<br>\n",
    "The [tf.zeros()](https://www.tensorflow.org/api_docs/python/tf/zeros) function returns a tensor with all zeros.\n",
    "\n",
    "8. **tf.nn.softmax() and cross-entropy**:<br>\n",
    "The softmax function squashes it's inputs, typically called logits or logit scores, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1. This means the output of the softmax function is equivalent to a categorical probability distribution. It's the perfect function to use as the output activation for a network predicting multiple classes.<br>\n",
    "Previously we've been using the sum of squared errors as the cost function in our networks, but in those cases we only have singular (scalar) output values.<br>\n",
    "When you're using **softmax**, however, your output is a vector. One vector is the probability values from the output units. You can also express your data labels as a vector using what's called **one-hot encoding**.<br> \n",
    "This just means that you have a vector the length of the number of classes, and the label element is marked with a 1 while the other labels are set to 0. In the case of classifying digits from before, our label vector for the image of the number 4 would be:\n",
    "```\n",
    "y=[0,0,0,0,1,0,0,0,0,0]\n",
    "```\n",
    "And our output prediction vector could be something like\n",
    "```\n",
    "y^=[0.047,0.048,0.061,0.07,0.330,0.062,0.001,0.213,0.013,0.150].\n",
    "```\n",
    "We want our error to be proportional to how far apart these vectors are. To calculate this distance, we'll use the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy). Then, our goal when training the network is to make our prediction vectors as close as possible to the label vectors by minimizing the cross entropy. The cross entropy calculation is shown below:\n",
    "```\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "```\n",
    "\n",
    "9. **Mini-Batching**:<br>\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.<br>\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.<br>\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
    "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's tf.placeholder() function to receive the varying batch sizes.\n",
    "```\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "```\n",
    "What does None do here?\n",
    "The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples.\n",
    "\n",
    "10. **Epochs**:<br>\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build a Classifier on the notMNIST dataset using the 10 basics Tensorflow API calls above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have a great library to build neural networks (better and easier than numpy), let's put all the above concepts to use and build a Single Layer (Input and Output) NN to predict/classify images!\n",
    "\n",
    "Click [here](Meetup8-Lab-notMNIST-Tensorflow.ipynb) to open the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
